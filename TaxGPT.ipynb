{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrOAsH10lb+28L3pjpg83B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8111d817abd04cbeb76bb21c1d82e387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_252f1adad2734cf0bba2b0057b63da82",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f2884961c1a4449b29a1348792fb53c",
            "value": 100
          }
        },
        "252f1adad2734cf0bba2b0057b63da82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "1f2884961c1a4449b29a1348792fb53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungkim11/TaxGPT/blob/main/TaxGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Python Packages"
      ],
      "metadata": {
        "id": "NW2gXJfsy3AS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8y9YCmu-Gxq",
        "outputId": "8adbb08e-a488-4c22-96f0-106ac51c46ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "openai\n",
        "chromadb\n",
        "tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOe3ZF6lev20",
        "outputId": "9d64ea29-060c-44d0-c165-021dbd010922"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.3.11-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai->-r requirements.txt (line 1)) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.9/dist-packages (from chromadb->-r requirements.txt (line 2)) (1.10.7)\n",
            "Collecting duckdb>=0.5.1\n",
            "  Downloading duckdb-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clickhouse-connect>=0.5.7\n",
            "  Downloading clickhouse_connect-0.5.16-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.2/925.2 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.9/dist-packages (from chromadb->-r requirements.txt (line 2)) (1.4.4)\n",
            "Collecting sentence-transformers>=2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi>=0.85.1\n",
            "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hnswlib>=0.7\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests>=2.20\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from chromadb->-r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->-r requirements.txt (line 3)) (2022.10.31)\n",
            "Collecting zstandard\n",
            "  Downloading zstandard-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4\n",
            "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3->chromadb->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (2.0.12)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (0.14.1+cu116)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2)) (8.1.3)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
            "  Downloading uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0\n",
            "  Downloading httptools-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (417 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 KB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2)) (6.0)\n",
            "Collecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting websockets>=10.4\n",
            "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13\n",
            "  Downloading watchfiles-0.18.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai->-r requirements.txt (line 1)) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.10.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3->chromadb->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (8.4.0)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: hnswlib, sentence-transformers\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp39-cp39-linux_x86_64.whl size=2122598 sha256=043b7f3aaec5f1e05ab2af51e4ce365497d1f753cd55d0cd2f3894ce52474e85\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/26/61/fface6c407f56418b3140cd7645917f20ba6b27d4e32b2bd20\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=70319ea0ef4771fc08666894393551dab69754d2b38c7ace914ae71db08f8da1\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built hnswlib sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, duckdb, zstandard, websockets, uvloop, sniffio, requests, python-dotenv, multidict, lz4, httptools, hnswlib, h11, frozenlist, async-timeout, yarl, uvicorn, tiktoken, huggingface-hub, clickhouse-connect, anyio, aiosignal, watchfiles, transformers, starlette, aiohttp, sentence-transformers, openai, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 anyio-3.6.2 async-timeout-4.0.2 chromadb-0.3.11 clickhouse-connect-0.5.16 duckdb-0.7.1 fastapi-0.95.0 frozenlist-1.3.3 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 huggingface-hub-0.13.3 lz4-4.3.2 multidict-6.0.4 openai-0.27.2 python-dotenv-1.0.0 requests-2.28.2 sentence-transformers-2.2.2 sentencepiece-0.1.97 sniffio-1.3.0 starlette-0.26.1 tiktoken-0.3.2 tokenizers-0.13.2 transformers-4.27.3 uvicorn-0.21.1 uvloop-0.17.0 watchfiles-0.18.1 websockets-10.4 yarl-1.8.2 zstandard-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Python Packages"
      ],
      "metadata": {
        "id": "x3I7L7FAy9Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "import chromadb\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "print('Python: ', platform.python_version())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_Iv9w1We3P1",
        "outputId": "c79ae7d2-cc89-4da5-f680-857ef026021c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Logger created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Chroma using direct local API.\n",
            "Using DuckDB in-memory for database. Data will be transient.\n",
            "Python:  3.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Storage - Google Drive"
      ],
      "metadata": {
        "id": "3uA_sQDdzBos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duz_YQdgfcA2",
        "outputId": "252b7bf7-d098-4d34-cf47-5cda06c686ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set OpenAI API Key"
      ],
      "metadata": {
        "id": "guDPqVN7zFPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"open ai api key\""
      ],
      "metadata": {
        "id": "130b-fkafhpN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "Lvckk4E00a53"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load ChromaDB"
      ],
      "metadata": {
        "id": "jzrkF64FzKjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import Settings\n",
        "client = chromadb.Client(Settings(\n",
        "    chroma_db_impl=\"duckdb+parquet\",\n",
        "    persist_directory=\"/content/drive/My Drive/Colab Notebooks/chromadb/tax/\" # Optional, defaults to .chromadb/ in the current directory\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "8111d817abd04cbeb76bb21c1d82e387",
            "252f1adad2734cf0bba2b0057b63da82",
            "1f2884961c1a4449b29a1348792fb53c"
          ]
        },
        "id": "6ZEs2Lnuek7Z",
        "outputId": "05c865d8-7540-4f49-a1d1-a4e430340384"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Chroma using direct local API.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8111d817abd04cbeb76bb21c1d82e387"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded in 59866 embeddings\n",
            "loaded in 2 collections\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = openai.Embedding()\n",
        "irc_collection = client.get_collection(name=\"irc\", embedding_function=embeddings)\n",
        "irb_collection = client.get_collection(name=\"irb\", embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "Tl9NUlDKfBaV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Functions"
      ],
      "metadata": {
        "id": "s_tqGbzizYRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
      ],
      "metadata": {
        "id": "YvlHIHobhCHV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def break_up_text_to_chunks(text, chunk_size=2000, overlap_size=100):\n",
        "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    tokens = encoding.encode(text)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, num_tokens, chunk_size - overlap_size):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        chunks.append(chunk)\n",
        "    \n",
        "    return chunks"
      ],
      "metadata": {
        "id": "lwOHlc3hWnVD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_prompt_text(tokenized_text):\n",
        "    prompt_text = \" \".join(tokenized_text)\n",
        "    prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
        "    return prompt_text"
      ],
      "metadata": {
        "id": "GsVKPEJy6Ohj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def askTaxGPT(question, debug = False):\n",
        "\n",
        "    #Change question to embeddings.\n",
        "    irc_question_ids = get_embedding(question)\n",
        "\n",
        "    #Query IRC collections.\n",
        "    irc_query_results = irc_collection.query(\n",
        "        query_embeddings=irc_question_ids,\n",
        "        n_results=10,\n",
        "        include=[\"documents\"]\n",
        "    )\n",
        "\n",
        "    #Join all items in a list\n",
        "    irc_documents = irc_query_results[\"documents\"][0]\n",
        "    irc_query_results_doc = \"\".join(irc_documents)\n",
        "\n",
        "    if debug == True:\n",
        "        print(irc_query_results_doc)\n",
        "\n",
        "    #For a given question, only return a list relevant Internal Revenue Codes that covers this topic.\n",
        "    prompt_response = []\n",
        "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "    chunks = break_up_text_to_chunks(irc_query_results_doc)\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt_request = question + \" Only return a list relevant Internal Revenue Codes that covers this topic.: \" + encoding.decode(chunks[i])\n",
        "        #prompt_request = question + \" Only return a list relevant Internal Revenue Codes that covers this topic.: \" + convert_to_prompt_text(chunks[i])\n",
        "        response = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=prompt_request,\n",
        "                temperature=0,\n",
        "                max_tokens=1000,\n",
        "                top_p=1,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0\n",
        "        )        \n",
        "        prompt_response.append(response[\"choices\"][0][\"text\"].strip())\n",
        "\n",
        "    #Consolidate a list relevant Internal Revenue Codes that covers this topic.\n",
        "    prompt_request = \"Consoloidate these a list of Internal Revenue Codes: \" + str(prompt_response)\n",
        "\n",
        "    if debug == True:\n",
        "        print(prompt_request)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",\n",
        "            prompt=prompt_request,\n",
        "            temperature=0,\n",
        "            max_tokens=1000,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0\n",
        "        )\n",
        "    \n",
        "    irc_codes = response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "    if debug == True:\n",
        "        print(prompt_request)\n",
        "\n",
        "    #Change question to embeddings.\n",
        "    irb_question_ids = get_embedding(question + irc_codes)\n",
        "\n",
        "    #Query IRB collections.\n",
        "    irb_query_results = irb_collection.query(\n",
        "        query_embeddings=irb_question_ids,\n",
        "        n_results=20,\n",
        "        include=[\"documents\"]\n",
        "    )\n",
        "\n",
        "    #Join all items in a list\n",
        "    irb_documents = irb_query_results[\"documents\"][0]\n",
        "    irb_query_results_doc = \"\".join(irb_documents)\n",
        "\n",
        "    if debug == True:\n",
        "        print(irb_query_results_doc)\n",
        "\n",
        "    #For a given question, provides answers, referencing I.R.C.\n",
        "    prompt_response = []\n",
        "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "    chunks = break_up_text_to_chunks(irb_query_results_doc)\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt_request = question + \" Cite I.R.C. as references.\" + encoding.decode(chunks[i])\n",
        "        #prompt_request = question + \" Cite I.R.C. as references.\" + convert_to_prompt_text(chunks[i])\n",
        "        response = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=prompt_request,\n",
        "                temperature=0,\n",
        "                max_tokens=1000,\n",
        "                top_p=1,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0\n",
        "        )        \n",
        "        prompt_response.append(response[\"choices\"][0][\"text\"].strip())\n",
        "\n",
        "    \n",
        "\n",
        "    if debug == True:\n",
        "        print(prompt_request)\n",
        "\n",
        "    #For a given question, provides answers, referencing I.R.C.\n",
        "    prompt_response = []\n",
        "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "    chunks = break_up_text_to_chunks(str(prompt_response))\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt_request = question + \" Cite I.R.C. as references.\" + encoding.decode(chunks[i])\n",
        "        response = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=prompt_request,\n",
        "                temperature=0,\n",
        "                max_tokens=1000,\n",
        "                top_p=1,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0            \n",
        "            )    \n",
        "        \n",
        "        return response[\"choices\"][0][\"text\"].strip()"
      ],
      "metadata": {
        "id": "8vGnq6C3z4A_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions and Answers"
      ],
      "metadata": {
        "id": "XfGbvk7g6YHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = askTaxGPT(\"I am USA citizen who is working for a USA-based company, but lived outside of USA for the calendar year. Do I still need to pay income tax?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2-vJYv04NTj",
        "outputId": "415045c9-2bf3-4870-cdce-9eff5a69c6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 4.291534423828125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0003650188446044922\n",
            "DEBUG:Chroma:time to pre process our knn query: 3.5762786865234375e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0005962848663330078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "utMc5hAU4xHR",
        "outputId": "d7a27b65-7be1-46e2-e9f2-9da3a401e28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In summary, a USA citizen who is working for a USA-based company and living outside of the USA for the calendar year is still required to pay income tax. The Internal Revenue Code (I.R.C.) does not exclude income derived in the U.S. from taxable income. The I.R.C. defines a taxpayer as any person subject to any internal revenue tax and further defines a person as an individual, trust, estate, partnership, association, company or corporation. Taxpayers are required to disclose foreign financial accounts to the Treasury Department and to report the income earned thereon. Taxpayers must report income earned as an individual on a Form 1040 and may not attribute the income to a trust created solely for the purpose of tax-avoidance, or claim deductions related to any expenses purportedly incurred by such a trust. Business expenses, including expenses related to a home-based business, are not deductible unless the expenses relate to a legitimate profit-seeking trade or business. Taxpayers are required to file returns and pay any tax owed. Additionally, taxpayers may be subject to additional taxes or penalties if they fail to comply with the I.R.C. and related regulations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = askTaxGPT(\"I am USA citizen who Lives in Colombia and is working for a USA-based company for the calendar year. I am required to pay income tax to Colombian government as part of my visa. Do I still need to pay income tax to USA?\", debug = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q13kO4uJ5Ke0",
        "outputId": "2db7df68-a549-4d57-d2af-670b5a31caf7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 3.5762786865234375e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0004699230194091797\n",
            "DEBUG:Chroma:time to pre process our knn query: 3.0994415283203125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0005600452423095703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "DvMxBWFh5fnH",
        "outputId": "ac413963-dfc6-41da-daf9-0289ca6a4d5b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, you are still required to pay income tax to the United States. According to the Internal Revenue Code (IRC) section 871(a), all income earned by a US citizen, regardless of where it is earned, is subject to US income tax. Additionally, IRC section 911(a) provides an exclusion from US income tax for certain foreign earned income, but this exclusion does not apply to income earned by a US citizen.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = askTaxGPT(\"What are the tax implications of exercising stock options or selling restricted stock units (RSUs)?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tJKWUQlzwtP",
        "outputId": "a09c280b-630b-4925-dde2-d6fa375ed778"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 4.291534423828125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.000978708267211914\n",
            "DEBUG:Chroma:time to pre process our knn query: 2.6226043701171875e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0006501674652099609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Gl3lMBljz2Rx",
        "outputId": "dd606ad9-89b6-4c18-a001-96dc18e3e8dc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Exercising stock options and selling restricted stock units (RSUs) both have tax implications. \\n\\nWhen exercising stock options, the difference between the exercise price and the fair market value of the stock at the time of exercise is considered ordinary income and is subject to income tax. This is known as the “bargain element” and is reported on Form 1099-MISC. The bargain element is also subject to employment taxes such as Social Security and Medicare taxes.\\n\\nWhen selling RSUs, the fair market value of the stock at the time of sale is considered ordinary income and is subject to income tax. This is reported on Form 1099-MISC. The sale of RSUs is also subject to employment taxes such as Social Security and Medicare taxes.\\n\\nThe Internal Revenue Code (IRC) sections that apply to the taxation of stock options and RSUs are sections 83 and 451. Section 83 of the IRC states that the difference between the exercise price and the fair market value of the stock at the time of exercise is considered ordinary income and is subject to income tax. Section 451 of the IRC states that the fair market value of the stock at the time of sale is considered ordinary income and is subject to income tax.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = askTaxGPT(\"How do I apply passive activity loss rules to my investments or business activities?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cgM3bXhBn_j",
        "outputId": "4a529e08-65c6-4de6-a827-d15ab091e62f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 4.76837158203125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.002870798110961914\n",
            "DEBUG:Chroma:time to pre process our knn query: 3.337860107421875e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.003595113754272461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "7rNSpujxBuI5",
        "outputId": "e886a595-c471-4822-b458-91dc3f48c08a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The passive activity loss rules are found in Internal Revenue Code (IRC) Section 469. Generally, these rules limit the amount of losses from passive activities that can be used to offset income from other sources. \\n\\nIn order to apply the passive activity loss rules, you must first determine whether the activity is a passive activity. A passive activity is any activity in which you do not materially participate. Generally, material participation is defined as any activity in which you are involved on a regular, continuous, and substantial basis. \\n\\nIf the activity is determined to be a passive activity, then you must determine whether the activity is a rental activity or a trade or business activity. If the activity is a rental activity, then the passive activity loss rules do not apply. However, if the activity is a trade or business activity, then the passive activity loss rules do apply. \\n\\nIf the activity is a trade or business activity, then you must determine whether the activity is a passive activity for the current tax year. If the activity is a passive activity for the current tax year, then the passive activity loss rules limit the amount of losses that can be used to offset income from other sources. Specifically, the amount of losses that can be used to offset income from other sources is limited to the amount of income generated from passive activities. \\n\\nFinally, if the activity is a passive activity for the current tax year, then you must determine whether the activity is a passive activity for the prior three tax years. If the activity is a passive activity for the prior three tax years, then the passive activity loss rules limit the amount of losses that can be used to offset income from other sources. Specifically, the amount of losses that can be used to offset income from other sources is limited to the amount of income generated from passive activities in the prior three tax years. \\n\\nReferences:\\n\\nInternal Revenue Code (IRC) Section 469'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}